{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import torch\n",
    "import torchaudio\n",
    "import tensorflow as tf\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display\n",
    "import moviepy.editor as mp\n",
    "import yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step Notebook for Voice Cloning\n",
    "\n",
    "# Step 1: Load the audio file\n",
    "# Load an audio file and set the sample rate\n",
    "def load_audio(file_path, sample_rate=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    return audio, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Features\n",
    "# Extract features such as MFCCs, spectral contrast, and chroma from the audio\n",
    "def extract_features(audio, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    return mfccs, spectral_contrast, chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the Features\n",
    "# Standardize the extracted features to make them comparable\n",
    "def preprocess_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = [scaler.fit_transform(feature.T).T for feature in features]\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load Pre-trained Model (e.g., Wav2Vec2 for Voice Representation)\n",
    "# Load the pre-trained Wav2Vec2 model and tokenizer\n",
    "def load_pretrained_model():\n",
    "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Convert Audio to Text Representation\n",
    "# Convert the audio input into text representation using the pre-trained model\n",
    "def audio_to_text_representation(audio_path, tokenizer, model):\n",
    "    audio_input, _ = torchaudio.load(audio_path)\n",
    "    input_values = tokenizer(audio_input.squeeze().numpy(), return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = tokenizer.decode(predicted_ids[0])\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Voice Synthesis (using a TTS model like Tacotron or Tensorspeech)\n",
    "# Synthesize voice from the given text using a Text-to-Speech model\n",
    "def synthesize_voice(text, tts_model):\n",
    "    audio = tts_model.tts(text)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Voice Cloning\n",
    "# Combine the synthesized voice features and the extracted features to make the cloned voice\n",
    "def clone_voice(features, synthesized_audio):\n",
    "    cloned_audio = synthesized_audio * 0.6 + features[0].mean() * 0.4  # Combining the original features with the new synthesized ones\n",
    "    return cloned_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute the pipeline in a step-by-step notebook format\n",
    "def main():\n",
    "    # Step 1: Load Audio\n",
    "    file_path = r'C:\\Users\\syrym\\Downloads\\research_2\\audio.wav'\n",
    "    audio, sr = load_audio(file_path)\n",
    "    display(Audio(data=audio, rate=sr))\n",
    "    print(\"Loaded Audio File: \", file_path)\n",
    "\n",
    "    # Step 2: Extract Features\n",
    "    mfccs, spectral_contrast, chroma = extract_features(audio, sr)\n",
    "    print(\"MFCCs Shape: \", mfccs.shape)\n",
    "    print(\"Spectral Contrast Shape: \", spectral_contrast.shape)\n",
    "    print(\"Chroma Shape: \", chroma.shape)\n",
    "\n",
    "    # Step 3: Preprocess Features\n",
    "    features = preprocess_features([mfccs, spectral_contrast, chroma])\n",
    "    print(\"Features Preprocessed.\")\n",
    "\n",
    "    # Step 4: Load Pre-trained Model\n",
    "    tokenizer, model = load_pretrained_model()\n",
    "    print(\"Loaded Pre-trained Wav2Vec2 Model.\")\n",
    "\n",
    "    # Step 5: Convert Audio to Text Representation\n",
    "    transcription = audio_to_text_representation(file_path, tokenizer, model)\n",
    "    print(\"Transcription: \", transcription)\n",
    "\n",
    "    # Step 6: Voice Synthesis (commented out as it requires a TTS model)\n",
    "    # tts_model = load_tts_model()  # Assuming you have a TTS model available\n",
    "    # synthesized_audio = synthesize_voice(transcription, tts_model)\n",
    "    # display(Audio(data=synthesized_audio, rate=sr))\n",
    "\n",
    "    # Step 7: Voice Cloning (commented out as it requires synthesized audio)\n",
    "    # cloned_voice = clone_voice(features, synthesized_audio)\n",
    "    # sf.write('cloned_voice.wav', cloned_voice, sr)\n",
    "    # display(Audio(data=cloned_voice, rate=sr))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
